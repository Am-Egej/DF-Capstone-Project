# ğŸ¾ Data Engineering - Tennis Project

Welcome to the Data Engineering - Tennis project! This repository contains the full pipeline for extracting, transforming, loading, testing, and visualizing professional tennis match data. Whether you're a data engineer, analyst, or tennis enthusiast, this project offers a hands-on look at building a real-world data product from scratch. 

---

## ğŸ¯ Project Goals

- Build a robust ETL pipeline for tennis match data
- Store and query data efficiently using PostgreSQL
- Create interactive visualizations for player insights and comparisons
- Practice production-grade data engineering workflows

---

## ğŸ› ï¸ Tech Stack

- **Python**
  - `pandas` â€“ Data manipulation and preprocessing  
  - `SQLAlchemy` â€“ ORM for PostgreSQL integration  
  - `psycopg2` / `psycopg2-binary` â€“ PostgreSQL connectivity  
  - `Streamlit` â€“ Interactive web app for visualizing tennis stats  
  - `Altair` & `Plotly` â€“ Declarative and interactive visualizations  
  - `kagglehub` â€“ Seamless access to Kaggle datasets via API  

- **PostgreSQL** â€“ Relational database for storing and querying tennis data  
- **Kaggle API** â€“ Programmatic access to tennis datasets  
- **Git & GitHub** â€“ Version control and collaboration

---

## ğŸ“¦ Project Overview

This project is organized into four main epics, each representing a key phase in the data pipeline:

1. **Extract** tennis data from Kaggle
2. **Transform** and clean the raw data
3. **Load** the processed data into PostgreSQL
4. **Visualize** player stats using a Streamlit app

---

## ğŸ§© Epics, User Stories & Tasks

### ğŸŸ¢ EPIC 1: Extract Tennis Data

#### User Stories
- As a data engineer, I want to programmatically download tennis datasets from Kaggle so I can begin preprocessing.


#### Tasks
- âœ… Set up Kaggle API credentials securely
- âœ… Write a Python script to download the dataset using the Kaggle API
- âœ… Unzip and store the dataset in a local or cloud directory
- âœ… Validate the integrity and completeness of the downloaded files

---

### ğŸŸ¡ EPIC 2: Transform and Clean Tennis Data

#### User Stories
- As a data engineer, I want to clean and standardize the dataset so itâ€™s ready for analysis and visualization.
- As an analyst, I want consistent player names and match formats to ensure accurate comparisons.
- As a data engineer, I want to extract tour results to analyze tour-specific performance.

#### Tasks
- âœ… Handle missing or null values
- âœ… Normalize player names (e.g., "GAUFF C." â†’ "Coco Gauff")
- âœ… Convert date strings to datetime objects and sort matches chronologically
- âœ… Engineer new features (e.g., set-level scores)
- âœ… Drop irrelevant or redundant columns
- âœ… Merge multiple datasets into a unified schema
- âœ… Write tests to verify transformation accuracy

---

### ğŸ”µ EPIC 3: Load Transformed Data into PostgreSQL

#### User Stories
- As a data engineer, I want to load the cleaned dataset into PostgreSQL so it can be queried efficiently.

#### Tasks
- âœ… Design PostgreSQL schema for tennis data
- âœ… Write ETL functions using `psycopg2` or SQLAlchemy
- âœ… Apply data types and constraints where relevant
- âœ… Validate successful data load with sample queries
- âœ… Document the database schema

---

### ğŸŸ£ EPIC 4: Visualize Player Stats via Streamlit

#### User Stories
- As a tennis fan, I want to select a player and view their performance trends over time.
- As a user, I want to compare two players side-by-side across key metrics.
- As a user, I want to explore leaderboards and current standings.
- As a user, I want to filter data by tour type, surface, etc.

#### Tasks
- âœ… Build Streamlit UI components for player selection and comparison
- âœ… Create time-series visualizations (e.g., win/loss trends, ranking progression)
- âœ… Generate leaderboard tables based on selected metrics
- âœ… Implement filters (e.g for tour type, surface, etc)
- âœ… Apply responsive design and styling
- âœ… Add tooltips and hover effects for charts
- âœ… Verify app functionality and performance

---
## ğŸ“ Folder structure - ETL Branch (etl_branch):
<pre>
df-capstone-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                                # Unprocessed files from Kaggle
â”‚   â”‚   â””â”€â”€ transformed_tennis_data.csv
â”‚   â””â”€â”€ processed/                          # Cleaned CSVs or intermediate outputs
â”‚       â”œâ”€â”€ atp_tennis.csv           
â”‚       â””â”€â”€ wta.csv     
â”œâ”€â”€ notebooks/                              # Data exploration notebook(s)
â”‚   â””â”€â”€ capstone_analysis.ipynb 
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py                         
â”‚   â”œâ”€â”€ config.py                           # DB credentials and constants
â”‚   â”œâ”€â”€ extract.py                          # Download and unzip Kaggle data                       
â”‚   â”œâ”€â”€ load.py                             # Load into PostgreSQL
â”‚   â””â”€â”€ transform.py                        # Clean and enrich data
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py                     # Tests for successful data extraction                     
â”‚   â”œâ”€â”€ load.py                             # Tests for successful data loading 
â”‚   â””â”€â”€ transform.py                        # Tests for successful data transformation 
â”œâ”€â”€ README.md                               # Project overview
â”œâ”€â”€ requirements.txt                        # Python dependencies
â””â”€â”€ run_etl.py                              # End to end run of ETL pipeline
</pre>

---

## ğŸ“ Folder structure - Streamlit Branch (main):
<pre>
df-capstone-project/
â”œâ”€â”€ .streamlit/          
â”‚   â””â”€â”€ secrets.toml                        # DB credentials and constants for local running of streamlit app
â”œâ”€â”€ data/     
â”‚   â”œâ”€â”€ updates/    
â”‚   â”‚   â”œâ”€â”€ postgreSQL_tennis_data.csv      # Table from PostgreSQL - Updated every run               
â”‚   â””â”€â”€ postgreSQL_tennis_data.csv          # Local CSV data for users without credentials (not updated)
â”œâ”€â”€ streamlit_scripts/
â”‚   â”œâ”€â”€ __init__.py                         
â”‚   â”œâ”€â”€ compare.py                          # Creates Player Comparisons visualisation
â”‚   â”œâ”€â”€ player.py                           # Creates Player insights visualiisation                  
â”‚   â”œâ”€â”€ rankings.py                         # Creates Rankings visualisation
â”‚   â”œâ”€â”€ sql_to_csv.py                       # Extarcts data from PostgreSQL and loads to CSV
â”‚   â””â”€â”€ welcome.py                          # Creates welcome visualisation 
â”œâ”€â”€ README.md                               # Project overview
â”œâ”€â”€ requirements.txt                        # Python dependencies
â””â”€â”€ tennis_streamlit.py                     # Creates full Streamlit app with all visualisations
</pre>
---

## ğŸš€ Getting Started

To run the project locally:

```bash
# Clone the repo
git clone https://github.com/Am-Egej/DF-Capstone-Project.git
cd df-capstone-project

# For ETL Pipeline
git switch etl_branch

# For Streamlit app
git switch main

# Install dependencies
pip install -r requirements.txt

# Run the ETL Pipeline
python run_etl.py

# Run the Streamlit app
streamlit run tennis_streamlit.py
```

---

## ğŸ§  Contributing

Beginner-friendly contributions are welcome! Feel free to open issues, suggest improvements, or submit pull requests. 

---

